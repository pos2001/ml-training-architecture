
```
GPT-2, GPT-3, Llama, Mixtral, DeepSeek, Minimax, Hunyuan 등 주요 대형 언어 모델의 파라미터(매개변수) 규모와 구조에 대한 객관적 정보와 변천사 정리
GPT-2 (2019년) 는 1.3억~16억 파라미터, GPT-3 (2020년) 는 1,750억(175B) 파라미터, Llama-3.1(2024년) 은 4050억(405B) 파라미터 등으로 대형 모델의 크기가 빠르게 증가함
MoE(전문가 혼합) 구조의 등장으로 GPT-3급 이상 모델이 오픈 소스/다운로드 가능하게 되었으며, 대표적으로 DeepSeek V3 Base(6710억), ERNIE-4.5(4240억), Mixtral-8x22B(1410억) 등 다양한 초대형 모델들이 등장함
Dense(모든 파라미터를 사용) 모델과 MoE(일부 전문가 파라미터만 활성화) 모델의 비교가 복잡해졌으며, 실질적인 "지능" 비교가 쉽지 않음
최근에는 다중 모달·다중 언어 지원, 새로운 아키텍처, 합성 데이터 활용 등 다양한 발전 트렌드가 나타남
이 문서는 최근 몇 년간 대형 언어 모델(LLM)의 기초 모델(베이스 모델) 규모 변화에 대한 사실 정보를 정리한 내용임
챗봇·어시스턴트가 아니라 텍스트 생성 엔진 본연의 모델에 초점을 맞추고 있음
역사
GPT-2(-medium, -large, -xl) (2019): 각각 1.37억, 3.8억, 8.12억, 16.1억 파라미터
약 40GB의 웹텍스트 데이터셋(10억 토큰 추정) 기반으로 훈련됨
사용된 사이트 목록은 domains.txt에서 확인 가능함
GPT-3(davinci, davinci-002) (2020): 1,750억 파라미터
CommonCrawl, WebText2, Books1·2, Wikipedia 등 약 4000억 토큰 데이터로 훈련됨
대규모 A100 GPU 수천 대로 수개월간 학습 필요
GPT-3.5, GPT-4 (2022, 2023): 아키텍처 및 데이터 관련 공식적 정보 비공개
Llama
Llama는 Meta(구 Facebook)에서 개발한 대형 언어 모델 시리즈로, 오픈 소스화와 상대적으로 적은 리소스로도 활용 가능한 구조로 주목받음
모델 크기(파라미터 수) 와 학습 데이터, 아키텍처의 진화 과정이 LLM(대형 언어 모델) 오픈소스 트렌드를 이끌었음
Llama 1 (2023)
7B, 13B, 33B, 65B: 70억, 130억, 330억, 650억 파라미터 제공
학습 데이터: 1.4조(1.4T) 토큰의 대규모 텍스트(Books3, CommonCrawl 등)
Llama 65B는 당시 오픈 모델 중 최대 규모였음
Books3는 저작권 관련 법제 논의의 중요한 계기가 된 대규모 데이터임
특징
상대적으로 작은 GPU로도 실행 가능(65B도 8장 GPU로 동작)
오픈 가중치 배포로, 다양한 파생 모델과 커뮤니티 실험 확산
Llama 2 (2023 하반기)
공개 당시 70억, 130억, 700억 파라미터 제공(7B, 13B, 70B)
대화형(챗봇) 버전도 공개, fine-tuning 및 RLHF(강화학습) 등 지원
커뮤니티와 상업적 용도까지 허용되는 라이선스(단, 일부 제한)
Llama 3.1 (2024)
405B: 4050억 dense(모든 파라미터 사용) 파라미터
학습 데이터: 2.87조 토큰 + 8000억 롱 컨텍스트 + 4000만 annealing(고품질 코드/수학 등 추가) → 총 3.67조 토큰
아키텍처
Transformer 기반, 모든 파라미터를 추론 과정에 동시 활용(dense)
고품질 코드·수학 데이터 추가로 주요 벤치마크 점수 극대화(annealing)
특징
대형 dense 모델 중 다운로드 가능한 최신 모델(오픈 소스)
Meta가 공개적으로 데이터셋 구성을 밝히지 않으며, 일부 저작권 논란 데이터(Books3 등) 포함 가능성 있음
일부 평가에선 "어시스턴트 성향"이 강화되어, 순수 텍스트 엔진으로서의 역할과 약간의 차이
Llama 4 (2025)
가장 큰 모델: 2조(2T) 파라미터 MoE(Mixture-of-Experts, 전문가 혼합 구조)
A288B 16E: 활성 2.88억 파라미터, 16개 전문가, 전체 2조 파라미터 중 일부만 활성화
상황
2T 모델은 미공개(내부 실험용), 파생/축소 버전만 외부 공개(maverick, scout 등)
파생 모델은 원본 대비 "지능"이 낮다는 평가가 많음
공개 과정에서 벤치마크 점수 조작 논란(lmarena 사건) 등으로 신뢰도 하락 및 팀 해체설
MoE 구조 특징
일부 전문가 파라미터만 활성화해, dense 모델보다 같은 파라미터 수 대비 연산 효율 우수
초대형 모델도 실사용 가능(분산 환경·적은 리소스에서 활용)
Llama의 의의 및 영향
Llama 시리즈는 오픈소스 생태계 확산과 대형 언어 모델의 대중화를 이끌었음
Llama-3.1 405B 공개를 기점으로 GPT-3/4급 대형 모델 다운로드/실험이 현실화
MoE 구조 도입으로 초대형 모델의 학습·배포가 활발해짐(DeepSeek, Mixtral 등에도 영향)
다만, 최근 모델들은 벤치마크 최적화(annealing), 어시스턴트 성향 강화 등으로 "순수 언어 모델"로서의 특성 변화 논의가 있음
The desert – 오픈소스 대형 모델의 공백기와 변화
GPT-3 수준(1,750억 파라미터급) 이상의 대형 언어 모델을 오픈소스로 구할 수 없던 긴 공백기를 의미함
이 시기(2020~2023년 중반)는 70B 이하 llama 등 비교적 작은 모델만 공개되어 있었고,
일부 프로젝트에서는 작은 Llama(예: 70B) 에 GPT-3가 생성한 합성 데이터로 파인튜닝하는 방식으로 성능을 끌어올리려 시도함
그러나 AI가 만든 텍스트를 다시 AI가 학습에 사용하면 데이터 품질 저하(데이터 "degeneration") 문제가 발생할 수 있음
GPT-3 수준의 오픈 가중치 모델이 장기간 부재했던 이유로,
학습 비용(수천~수만 개 GPU 인프라), 데이터 확보, 대형 파라미터 구조의 배포 난이도 등이 복합적으로 작용
Llama-3.1 405B(4050억 dense 파라미터) 모델이 공개되면서 본격적으로 초대형 모델의 오픈소스화가 시작됨
그 직전(2023년 12월) Mistral의 Mixtral-8x7B(MoE 구조, 총 560억 파라미터), 2024년 4월 Mixtral-8x22B(총 1,410억, 활성 390억 파라미터) 등
MoE(전문가 혼합) 아키텍처를 활용해 GPT-3급 대형 모델을 비교적 적은 리소스로 훈련·배포 가능하게 만듦
MoE 구조는 여러 전문가 네트워크(Expert)를 두고, 한 번의 추론 시 일부만 활성화
이를 통해 dense 구조보다 적은 리소스(메모리·연산)로 대형 모델 운영이 가능함
GPU 대수와 메모리 한계로 인해, MoE는 대형 오픈모델 대중화에 결정적 역할을 함
최신 MoE(전문가 혼합) 대형 모델
Deepseek V3 Base (2024)
6,710억 파라미터(MoE), 활성 370억, 14.8조 고품질 토큰 학습
R1(추론 특화 모델)도 등장, 다운로드 가능 모델 중 최초로 GPT-4급 성능에 근접
공개 직후 NVIDIA(NVDA) 주가가 일시 하락하는 등 시장에 큰 영향을 줌
이후 중국계를 포함한 신흥 대형 MoE 모델들이 속속 출현
일부 모델은 멀티모달·다국어 지원을 위해 다양한 유형의 새로운 데이터를 학습에 도입함
Databricks (DBRX, 2024년 3월)
1,320억 총 파라미터, 활성 360억, 12조 토큰
16개 전문가 중 4개 선택(미스트랄·그록 대비 더 세분화)
Minimax (2025년 1월)
4560억 총 파라미터, 활성 459억, 자체 reward labeler로 학습 데이터 품질 제어
Dots (2025년 6월)
1,430억 총 파라미터, 활성 140억, 11.2조 토큰, 32K 컨텍스트
top-6/128 전문가 구조, Qwen2.5-72B와 유사 성능
Hunyuan (2025년 6월)
800억 MoE, 활성 130억, 20조 토큰, 256K 컨텍스트
8개의 비공유 전문가 활성, 공유 전문가 항상 활성
Ernie (2025년 6월)
4,240억 총 파라미터, 활성 470억, 수조 토큰
결론 및 전망
2024~2025년 기준, GPT-3급(1,750억) 이상 초대형 모델이 다양하게 공개되고 있음
405B(4050억)가 최신 dense base 모델이지만, 최신 MoE 모델들도 대형화·다양화 중
Dense vs MoE 성능 비교는 아직 모호, 진정한 "지능"에 필요한 구조와 크기에 대한 논의 필요
새로운 구조(RWKV, byte-latent, bitnet), 합성 데이터 활용 등도 실험되고 있으나, 순수 텍스트 엔진으로서의 본질적 발전은 여전히 과제
최근 대부분의 대형 모델이 "AI 어시스턴트" 역할로 fine-tune되는 경향, 대안적 LLM 탐구가 필요한 시점
```
