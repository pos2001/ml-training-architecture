
### Flash-attn은 PyTorch 내에서 사용되는 커널 수준 최적화 라이브러리
```
┌─────────────────────────────────────────────────────────────────────┐
│                    APPLICATION CODE                                  │
│                      (train.py)                                      │
│                                                                       │
│  model = Qwen3MoeForCausalLM.from_pretrained(...)                   │
│  # Flash Attention은 내부적으로 자동 사용됨                         │
└─────────────────────────────────────────────────────────────────────┘
                            ↓
┌─────────────────────────────────────────────────────────────────────┐
│              DISTRIBUTED TRAINING LIBRARIES                          │
│                                                                       │
│  model = deepspeed.initialize(model, ...)                           │
└─────────────────────────────────────────────────────────────────────┘
                            ↓
┌─────────────────────────────────────────────────────────────────────┐
│              DEEP LEARNING FRAMEWORKS                                │
│                   (PyTorch)                                          │
│                                                                       │
│  ┌────────────────────────────────────────────────────┐             │
│  │  MODEL ARCHITECTURES                               │             │
│  │  • Qwen3MoeForCausalLM  ← 모델 아키텍처           │             │
│  └────────────────────────────────────────────────────┘             │
│                            ↓                                         │
│  ┌────────────────────────────────────────────────────┐             │
│  │  OPTIMIZED KERNELS (최적화된 커널)                │             │
│  │  • Flash Attention 2.8.3  ← 여기!                 │             │
│  │  • xFormers                                        │             │
│  │  • Triton kernels                                  │             │
│  │  • torch.nn.functional (기본 구현)                │             │
│  └────────────────────────────────────────────────────┘             │
│                            ↓                                         │
│  • CUDA kernels                                                      │
│  • cuBLAS, cuDNN                                                     │
└─────────────────────────────────────────────────────────────────────┘

```




### Qwen3-0.6B는 모델이 매우 작기 때문에 고급 분산 학습 기법이 불필요하다는 뜻입니다.
### 모델 크기와 필요한 분산 학습 기법
```
모델 크기에 따른 전략:

┌─────────────────────────────────────────────────────────────┐
│  0.6B 파라미터 (Qwen3-0.6B)                                 │
│  메모리: ~2.4GB (FP16/BF16)                                  │
│  전략: Data Parallel (DDP)만으로 충분 ✅                    │
│  이유: 단일 GPU에 모델 전체가 들어감                        │
└─────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────┐
│  7B 파라미터 (Llama-7B, Qwen-7B)                            │
│  메모리: ~28GB (FP16/BF16)                                   │
│  전략: DDP 또는 FSDP                                         │
│  이유: A100 80GB에 들어가지만 여유 적음                     │
└─────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────┐
│  70B 파라미터 (Llama-70B)                                   │
│  메모리: ~280GB (FP16/BF16)                                  │
│  전략: DeepSpeed ZeRO Stage 2/3 필요 ⚠️                     │
│  이유: 단일 GPU에 절대 들어갈 수 없음                       │
└─────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────┐
│  175B+ 파라미터 (GPT-3, GPT-4)                              │
│  메모리: ~700GB+ (FP16/BF16)                                 │
│  전략: DeepSpeed ZeRO Stage 3 + Tensor/Pipeline Parallel 필수│
│  이유: 여러 GPU에 모델을 나눠야 함                          │
└─────────────────────────────────────────────────────────────┘

```





###
```

```





###
```

```





###
```

```





###
```

```





###
```

```





###
```

```





###
```

```




###
```

```
