
###Megatron 분산 학습 프로세스 아키텍처, 전체 구조 (16 GPUs = 16 프로세스)
```
┌─────────────────────────────────────────────────────────────────┐
│                    SLURM JOB (srun/mpirun)                      │
│                     총 16개 프로세스 생성                         │
└─────────────────────────────────────────────────────────────────┘
                              │
                ┌─────────────┴─────────────┐
                │                           │
        ┌───────▼────────┐          ┌───────▼────────┐
        │  COMPUTE NODE 1 │          │  COMPUTE NODE 2 │
        │  (8 프로세스)    │          │  (8 프로세스)    │
        └────────────────┘          └────────────────┘

```

### Node 1: 8개 프로세스 (Rank 0-7)
```
┌─────────────────────────────────────────────────────────────────┐
│                     COMPUTE NODE 1                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  프로세스 Rank 0 (PID: 12345)                                   │
│  ┌──────────────────────────────────────────────────────┐      │
│  │  Container: megatron-training                         │      │
│  │  ├─ Python 인터프리터                                 │      │
│  │  ├─ PyTorch 프로세스                                  │      │
│  │  ├─ CUDA Context (GPU 0 전용)                        │      │
│  │  ├─ NCCL Communicator (Rank 0)                       │      │
│  │  │   └─ EFA 연결: Rank 0 ↔ Rank 1-15               │      │
│  │  └─ 메모리:                                           │      │
│  │      ├─ GPU 메모리: 모델 파라미터 일부               │      │
│  │      └─ CPU 메모리: 데이터 로딩, 버퍼               │      │
│  └──────────────────────────────────────────────────────┘      │
│                         ↓ 전용 연결                             │
│                      GPU 0 (H100)                               │
│                                                                 │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  프로세스 Rank 1 (PID: 12346)                                   │
│  ┌──────────────────────────────────────────────────────┐      │
│  │  Container: megatron-training                         │      │
│  │  ├─ Python 인터프리터                                 │      │
│  │  ├─ PyTorch 프로세스                                  │      │
│  │  ├─ CUDA Context (GPU 1 전용)                        │      │
│  │  ├─ NCCL Communicator (Rank 1)                       │      │
│  │  └─ GPU 메모리: 모델 파라미터 일부                   │      │
│  └──────────────────────────────────────────────────────┘      │
│                         ↓                                       │
│                      GPU 1 (H100)                               │
│                                                                 │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  프로세스 Rank 2-7 (PID: 12347-12352)                           │
│  └─ 각각 GPU 2-7에 전용 연결                                    │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘

```

## 프로세스 간 통신 구조
### 1. 같은 노드 내 통신 (Rank 0 ↔ Rank 1-7)
```
┌─────────────────────────────────────────────────────────┐
│  프로세스 Rank 0                프로세스 Rank 1         │
│  ┌──────────────┐                ┌──────────────┐      │
│  │ NCCL Send    │                │ NCCL Recv    │      │
│  └──────┬───────┘                └──────▲───────┘      │
│         │                               │              │
│         │ NVLink (900 GB/s)             │              │
│         │                               │              │
│      GPU 0 ◄──────────────────────────► GPU 1         │
│  (H100 메모리)                     (H100 메모리)       │
│                                                         │
│  통신 경로: GPU 메모리 → NVLink → GPU 메모리            │
│  (CPU 메모리 거치지 않음 = Zero-copy)                   │
└─────────────────────────────────────────────────────────┘

```
### 2. 다른 노드 간 통신 (Rank 0 ↔ Rank 8)
```
┌──────────────────────┐           ┌──────────────────────┐
│  Node 1: Rank 0      │           │  Node 2: Rank 8      │
│  ┌────────────────┐  │           │  ┌────────────────┐  │
│  │ NCCL Send      │  │           │  │ NCCL Recv      │  │
│  └────────┬───────┘  │           │  └────────▲───────┘  │
│           │          │           │           │          │
│           ↓          │           │           ↑          │
│       GPU 0          │           │       GPU 0          │
│           │          │           │           │          │
│           │ GPUDirect RDMA       │           │          │
│           ↓          │           │           ↑          │
│       EFA NIC        │           │       EFA NIC        │
│           │          │           │           │          │
│           └──────────┼───────────┼───────────┘          │
│                      │  100 Gbps │                      │
│                      │  EFA Link │                      │
└──────────────────────┘           └──────────────────────┘

통신 경로: GPU 메모리 → EFA NIC → 네트워크 → EFA NIC → GPU 메모리
(CPU 메모리 거치지 않음 = GPUDirect RDMA)

```




### 실제 프로세스 실행 흐름
```
1. 작업 제출
srun --nodes=2 --ntasks=16 --gpus-per-node=8 \
     --container-image=/fsx/megatron-training.sqsh \
     python -m torch.distributed.launch \
     --nproc_per_node=8 \
     pretrain_gpt.py

2. Slurm이 하는 일
Slurm (srun)
├─ Node 1에 8개 태스크 할당
│  ├─ Task 0 → GPU 0
│  ├─ Task 1 → GPU 1
│  ├─ ...
│  └─ Task 7 → GPU 7
│
└─ Node 2에 8개 태스크 할당
   ├─ Task 8 → GPU 0
   ├─ Task 9 → GPU 1
   ├─ ...
   └─ Task 15 → GPU 7

3. 각 프로세스가 하는 일
# 프로세스 Rank 0 (예시)

import torch
import torch.distributed as dist

# 1. 분산 초기화
dist.init_process_group(
    backend='nccl',        # NCCL 사용
    rank=0,                # 내 Rank
    world_size=16          # 전체 프로세스 수
)

# 2. GPU 할당
torch.cuda.set_device(0)   # GPU 0 전용

# 3. 모델 로드 (내 파라미터 일부만)
model = MegatronGPT(...)
model = model.to('cuda:0')

# 4. 데이터 로드 (내 배치만)
dataloader = get_dataloader(rank=0, world_size=16)

# 5. 학습 루프
for batch in dataloader:
    # Forward pass
    output = model(batch)
    loss = compute_loss(output)
    
    # Backward pass
    loss.backward()
    
    # Gradient 동기화 (NCCL All-Reduce)
    # → 다른 15개 프로세스와 통신
    dist.all_reduce(model.parameters())
    
    # Optimizer step
    optimizer.step()
```

### 프로세스 메모리 구조
```
┌─────────────────────────────────────────────────────────┐
│  프로세스 메모리 공간 (Rank 0)                           │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  CPU 메모리 (시스템 RAM)                                 │
│  ┌───────────────────────────────────────────────┐     │
│  │  - Python 인터프리터                           │     │
│  │  - PyTorch 코드                                │     │
│  │  - 데이터 로더 버퍼                            │     │
│  │  - NCCL 통신 버퍼                              │     │
│  │  - 로그, 체크포인트 버퍼                       │     │
│  └───────────────────────────────────────────────┘     │
│                                                         │
│  GPU 메모리 (H100 GPU 0)                                │
│  ┌───────────────────────────────────────────────┐     │
│  │  - 모델 파라미터 (1/16)                       │     │
│  │  - Gradient 버퍼                               │     │
│  │  - Optimizer 상태                              │     │
│  │  - 활성화(Activation) 값                      │     │
│  │  - 임시 텐서                                   │     │
│  └───────────────────────────────────────────────┘     │
│                                                         │
└─────────────────────────────────────────────────────────┘

```



## 병렬화 전략 (프로세스 관점)
###  Data Parallel (DP)
```
모든 프로세스가 전체 모델을 가짐
각 프로세스가 다른 데이터 배치 처리

Rank 0: 모델(전체) + 배치 0
Rank 1: 모델(전체) + 배치 1
...
Rank 15: 모델(전체) + 배치 15

Gradient 동기화: All-Reduce (모든 프로세스 참여)

```

### Tensor Parallel (TP)
```
모델을 레이어별로 분할
각 프로세스가 모델의 일부만 가짐

예: 4-way TP
Rank 0-3: 레이어 1의 1/4씩
Rank 4-7: 레이어 2의 1/4씩
...

통신: All-Gather, Reduce-Scatter

```

### Pipeline Parallel (PP)

```
모델을 레이어 단위로 분할
각 프로세스가 연속된 레이어 담당

Rank 0-3: Layer 1-10
Rank 4-7: Layer 11-20
Rank 8-11: Layer 21-30
Rank 12-15: Layer 31-40

통신: Point-to-Point (파이프라인 단계 간)

```

### 요약
```
네, 맞습니다! 학습은 프로세스가 합니다:

    ✅ 16개 프로세스 = 16개 독립적인 Python 실행
    ✅ 각 프로세스 = 1개 GPU 전용
    ✅ 프로세스 간 통신 = NCCL (NVLink + EFA)
    ✅ 각 프로세스 = 모델의 일부 + 데이터의 일부
    ✅ 동기화 = All-Reduce, All-Gather 
```



