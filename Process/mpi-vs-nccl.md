

```
MPI 환경:
└── 프로세스 주체: CPU 코어
    └── 각 CPU 코어가 MPI 프로세스 실행

NCCL 환경:
└── 프로세스 주체: GPU
    └── 각 GPU가 NCCL 통신 참여
```

##  MPI 환경 (CPU 기반)
```
┌─────────────────────────────────────────────────────────────┐
│              MPI 환경 (CPU 병렬)                            │
└─────────────────────────────────────────────────────────────┘

노드 1 (hpc6a - 96 CPU 코어)
┌─────────────────────────────────────────────────────────┐
│  CPU 코어                                               │
│  ┌────┐ ┌────┐ ┌────┐ ┌────┐       ┌────┐            │
│  │코어│ │코어│ │코어│ │코어│  ...  │코어│            │
│  │ 0  │ │ 1  │ │ 2  │ │ 3  │       │ 95 │            │
│  └─┬──┘ └─┬──┘ └─┬──┘ └─┬──┘       └─┬──┘            │
│    │      │      │      │             │               │
│    ↓      ↓      ↓      ↓             ↓               │
│  ┌────┐ ┌────┐ ┌────┐ ┌────┐       ┌────┐            │
│  │MPI │ │MPI │ │MPI │ │MPI │  ...  │MPI │            │
│  │ 0  │ │ 1  │ │ 2  │ │ 3  │       │ 95 │            │
│  └────┘ └────┘ └────┘ └────┘       └────┘            │
│                                                        │
│  프로세스 주체: CPU 코어 ⭐                            │
└─────────────────────────────────────────────────────────┘

특징:
├── CPU 코어가 프로세스 실행
├── 각 코어가 독립적인 MPI 프로세스
├── 메모리: 시스템 RAM 사용
└── 통신: MPI (네트워크 또는 공유 메모리)

```

### NCCL 환경 (GPU 기반)

```
┌─────────────────────────────────────────────────────────────┐
│              NCCL 환경 (GPU 병렬)                           │
└─────────────────────────────────────────────────────────────┘

노드 1 (p5.48xlarge - 8 GPU)
┌─────────────────────────────────────────────────────────┐
│  GPU                                                    │
│  ┌────┐ ┌────┐ ┌────┐ ┌────┐ ┌────┐ ┌────┐ ┌────┐ ┌────┐│
│  │GPU │ │GPU │ │GPU │ │GPU │ │GPU │ │GPU │ │GPU │ │GPU ││
│  │ 0  │ │ 1  │ │ 2  │ │ 3  │ │ 4  │ │ 5  │ │ 6  │ │ 7  ││
│  │H100│ │H100│ │H100│ │H100│ │H100│ │H100│ │H100│ │H100││
│  └─┬──┘ └─┬──┘ └─┬──┘ └─┬──┘ └─┬──┘ └─┬──┘ └─┬──┘ └─┬──┘│
│    │      │      │      │      │      │      │      │    │
│    ↓      ↓      ↓      ↓      ↓      ↓      ↓      ↓    │
│  ┌────┐ ┌────┐ ┌────┐ ┌────┐ ┌────┐ ┌────┐ ┌────┐ ┌────┐│
│  │NCCL│ │NCCL│ │NCCL│ │NCCL│ │NCCL│ │NCCL│ │NCCL│ │NCCL││
│  │Rank│ │Rank│ │Rank│ │Rank│ │Rank│ │Rank│ │Rank│ │Rank││
│  │ 0  │ │ 1  │ │ 2  │ │ 3  │ │ 4  │ │ 5  │ │ 6  │ │ 7  ││
│  └────┘ └────┘ └────┘ └────┘ └────┘ └────┘ └────┘ └────┘│
│                                                            │
│  프로세스 주체: GPU ⭐                                     │
└─────────────────────────────────────────────────────────────┘

특징:
├── GPU가 통신 주체
├── 각 GPU가 독립적인 NCCL Rank
├── 메모리: GPU 메모리 (HBM) 사용
└── 통신: NCCL (NVLink, PCIe, 네트워크)

```

### 상세비교표
```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    MPI vs NCCL 비교                                          │
└─────────────────────────────────────────────────────────────────────────────┘

항목                  │ MPI (CPU)              │ NCCL (GPU)
─────────────────────┼────────────────────────┼──────────────────────
프로세스 주체         │ CPU 코어 ⭐            │ GPU ⭐
실행 단위             │ MPI 프로세스           │ NCCL Rank
하드웨어              │ CPU (x86, ARM)         │ GPU (NVIDIA)
─────────────────────┼────────────────────────┼──────────────────────
메모리                │ 시스템 RAM             │ GPU 메모리 (HBM)
메모리 크기           │ 384 GB (hpc6a)         │ 80 GB/GPU (H100)
─────────────────────┼────────────────────────┼──────────────────────
통신 라이브러리       │ MPI (OpenMPI, Intel)   │ NCCL (NVIDIA)
노드 내 통신          │ 공유 메모리 (shm)      │ NVLink (900 GB/s)
노드 간 통신          │ EFA (100 Gbps)         │ EFA (3200 Gbps)
─────────────────────┼────────────────────────┼──────────────────────
주요 용도             │ HPC (과학 계산)        │ AI/ML (딥러닝)
대표 애플리케이션     │ WRF, GROMACS          │ PyTorch, TensorFlow
계산 특성             │ 범용 계산              │ 행렬 연산, 텐서 연산
─────────────────────┼────────────────────────┼──────────────────────
프로그래밍            │ C/Fortran + MPI        │ Python/C++ + CUDA
병렬 모델             │ 프로세스 기반          │ GPU 기반

```




## 하이브리드 구성 (MPI + NCCL)
### 대규모 AI 학습에서는 둘 다 사용:
```
┌─────────────────────────────────────────────────────────────┐
│          하이브리드: MPI + NCCL                             │
└─────────────────────────────────────────────────────────────┘

노드 1 (8 GPU)                    노드 2 (8 GPU)
┌──────────────────┐              ┌──────────────────┐
│ CPU 프로세스 0   │              │ CPU 프로세스 1   │
│ (MPI Rank 0)     │              │ (MPI Rank 1)     │
│                  │              │                  │
│ ┌──────────────┐ │              │ ┌──────────────┐ │
│ │ GPU 0 (NCCL) │ │              │ │ GPU 0 (NCCL) │ │
│ │ GPU 1 (NCCL) │ │              │ │ GPU 1 (NCCL) │ │
│ │ GPU 2 (NCCL) │ │              │ │ GPU 2 (NCCL) │ │
│ │ GPU 3 (NCCL) │ │              │ │ GPU 3 (NCCL) │ │
│ │ GPU 4 (NCCL) │ │              │ │ GPU 4 (NCCL) │ │
│ │ GPU 5 (NCCL) │ │              │ │ GPU 5 (NCCL) │ │
│ │ GPU 6 (NCCL) │ │              │ │ GPU 6 (NCCL) │ │
│ │ GPU 7 (NCCL) │ │              │ │ GPU 7 (NCCL) │ │
│ └──────────────┘ │              │ └──────────────┘ │
└────────┬─────────┘              └─────────┬────────┘
         │                                  │
         │        MPI 통신 (노드 간)        │
         └──────────────┬───────────────────┘
                        │
                   EFA 네트워크

구조:
├── CPU 프로세스: MPI로 관리 (노드 간 통신)
└── GPU: NCCL로 관리 (GPU 간 통신)
    ├── 노드 내: NVLink (NCCL)
    └── 노드 간: EFA (MPI + NCCL)

```
