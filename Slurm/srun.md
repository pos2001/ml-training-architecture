
## slurm 환경에서
```
┌─────────────────────────────────────────────────┐
│              작업 제출 (sbatch)                  │
└─────────────────────────────────────────────────┘
                    ↓
        ┌───────────────────────┐
        │   어떤 명령어 사용?    │
        └───────────────────────┘
           ↙                  ↘
    ┌─────────┐          ┌─────────┐
    │  srun   │          │ mpirun  │
    └─────────┘          └─────────┘
        ↓                     ↓
┌──────────────┐      ┌──────────────┐
│ Slurm 네이티브 │      │ MPI 직접 실행   │
│ 리소스 관리    │      │ Slurm 우회     │
│ 더 나은 통합   │      │ 호환성 우선      │
└──────────────┘      └──────────────┘

```
<img width="803" height="303" alt="image" src="https://github.com/user-attachments/assets/3819bab5-272c-4877-909e-bd816819a8da" />




### 권장 사용 가이드
```
┌─────────────────────────────────────────────────────────────┐
│ mpirun 사용을 권장하는 경우                                  │
└─────────────────────────────────────────────────────────────┘
✓ 전용 클러스터 (단일 사용자)
✓ 빠른 프로토타이핑
✓ 특정 MPI 구현 기능 필요
✓ 레거시 워크플로우 유지
✓ Slurm 설치 없는 환경

┌─────────────────────────────────────────────────────────────┐
│ srun 사용을 권장하는 경우 (대부분의 경우)                    │
└─────────────────────────────────────────────────────────────┘
✓ 공유 클러스터 (다중 사용자)
✓ 리소스 관리 필요
✓ 작업 큐잉 및 스케줄링
✓ 컨테이너 기반 워크플로우
✓ GPU 자동 할당
✓ 로깅 및 모니터링
✓ 장기 운영 환경


mpirun:
"GPU 8개 써야지!"
→ 실제로 사용 가능한지 모름
→ 다른 사람이 쓰고 있으면 충돌

srun:
"GPU 8개 주세요"
→ Slurm: "지금 4개만 가능합니다. 대기하시겠어요?"
→ 사용자: "네"
→ 4개 비면 자동으로 시작

```


## --mpi=pmix 옵션이 필요한 경우 vs 불필요한 경우
```
┌─────────────────────────────────────────────────────────┐
│ --mpi=pmix 필요 (MPI 프로그램)                          │
└─────────────────────────────────────────────────────────┘
✓ MPI_Init()을 호출하는 프로그램
✓ mpicc/mpicxx로 컴파일한 프로그램
✓ 프로세스 간 MPI 통신이 필요한 경우

예시:
srun --mpi=pmix ./mpi_program        ← 필요!

┌─────────────────────────────────────────────────────────┐
│ --mpi=pmix 불필요 (비MPI 프로그램)                      │
└─────────────────────────────────────────────────────────┘
✓ 일반 프로그램 (MPI 사용 안 함)
✓ PyTorch/TensorFlow (자체 통신 사용)
✓ 독립적인 병렬 작업

예시:
srun python train.py                 ← 불필요
srun ./standalone_program            ← 불필요

```

### A. MPI 프로그램 (--mpi=pmix 필요)

```
# ✅ 올바른 사용
srun --nodes=4 \
--ntasks-per-node=8 \
--mpi=pmix \
./mpi_simulation

# ❌ --mpi=pmix 없이 실행하면?
srun --nodes=4 \
--ntasks-per-node=8 \
./mpi_simulation

# 결과:
# - MPI_Init() 실패 또는 행(hang)
# - 에러: "PMI2_Init failed"
# - 프로세스들이 서로를 찾지 못함


## 왜 필요한가?
--mpi=pmix가 하는 일:

1. PMIx 서버 시작
└─ 각 노드에서 프로세스 관리 데몬 실행

2. 환경 변수 설정
├─ PMI_RANK (프로세스 순번)
├─ PMI_SIZE (전체 프로세스 수)
└─ PMIX_* 관련 변수들

3. 프로세스 간 연결 정보 제공
└─ 각 프로세스가 다른 프로세스를 찾을 수 있게 함

4. MPI_Init()이 성공적으로 완료됨

```

### PyTorch 분산 학습 (--mpi=pmix 불필요)
```
# ✅ PyTorch는 자체 통신 사용
srun --nodes=4 \
--ntasks-per-node=8 \
--gpus-per-task=1 \
python -m torch.distributed.launch train.py

# 또는
srun --nodes=4 \
--ntasks-per-node=8 \
--gpus-per-task=1 \
torchrun --nnodes=4 --nproc_per_node=8 train.py

# --mpi=pmix 없어도 작동!
# 왜? PyTorch는 NCCL/Gloo를 직접 사용


PyTorch 분산 통신:
├─ NCCL (GPU 간 통신)
│  └─ MPI 불필요, 직접 RDMA 사용
├─ Gloo (CPU 간 통신)
│  └─ TCP/IP 소켓 직접 사용
└─ 환경 변수로 프로세스 조정
├─ MASTER_ADDR
├─ MASTER_PORT
├─ WORLD_SIZE
└─ RANK (Slurm이 자동 설정)

```








### mpirun을 쓰는 이유 
```
1. 전용 클러스터 (단일 사용자)
상황: 집에 컴퓨터 4대가 있고, 나 혼자만 사용

mpirun 사용:
"컴퓨터1, 너는 작업 A 해"
"컴퓨터2, 너는 작업 B 해"
→ 간단하고 빠름!

srun 사용:
"관리자님, 컴퓨터 좀 써도 될까요?"
"네, 허가합니다. 컴퓨터1, 2 사용하세요"
→ 나 혼자 쓰는데 굳이 허락받을 필요 없음

왜? 혼자 쓰는 환경에서는 복잡한 관리 시스템이 불필요


2. 빠른 프로토타이핑
개발 중인 코드 테스트:

mpirun:
$ mpirun -np 4 ./test_program
→ 바로 실행! (2초)

srun:
$ srun --nodes=1 --ntasks=4 ./test_program
→ Slurm 스케줄러 대기... (5-10초)
→ 리소스 할당...
→ 실행

작은 테스트를 100번 반복한다면?
mpirun: 200초
srun: 500-1000초
```

#





### mpirun = MPI 전용 프로세스 런처    
```
사용자 명령어:
mpirun -np 32 -hostfile hosts.txt ./mpi_program

┌─────────────────────────────────────────────────────────────┐
│ mpirun (마스터 프로세스)                                     │
│ ├─ hostfile 파싱                                            │
│ ├─ 노드 목록 읽기                                           │
│ └─ SSH/RSH로 원격 노드 접속                                 │
└─────────────────────────────────────────────────────────────┘
↓
┌───────────────────────────────────────────────┐
│ Node 1 (8 프로세스)                           │
│ ┌──────────────────────────────────────────┐ │
│ │ mpirun이 SSH로 직접 프로세스 시작        │ │
│ │                                           │ │
│ │ Rank 0: ./mpi_program                    │ │
│ │ Rank 1: ./mpi_program                    │ │
│ │ Rank 2: ./mpi_program                    │ │
│ │ ...                                       │ │
│ │ Rank 7: ./mpi_program                    │ │
│ └──────────────────────────────────────────┘ │
└───────────────────────────────────────────────┘

┌───────────────────────────────────────────────┐
│ Node 2 (8 프로세스)                           │
│ ┌──────────────────────────────────────────┐ │
│ │ Rank 8:  ./mpi_program                   │ │
│ │ Rank 9:  ./mpi_program                   │ │
│ │ ...                                       │ │
│ │ Rank 15: ./mpi_program                   │ │
│ └──────────────────────────────────────────┘ │
└───────────────────────────────────────────────┘

[Node 3, Node 4도 동일]

특징:
✗ 리소스 관리 없음 (수동으로 노드 지정)
✗ 작업 큐잉 없음
✗ 다른 사용자와 충돌 가능
✓ MPI 특화 기능 풍부

```


```
┌──────────────────────────────────────────────────────────────┐
│ 1. mpirun 실행                                               │
└──────────────────────────────────────────────────────────────┘
mpirun -np 32 -hostfile hosts.txt ./mpi_program
↓
┌──────────────────────────────────────────────────────────────┐
│ 2. hostfile 파싱                                             │
└──────────────────────────────────────────────────────────────┘
hosts.txt:
node1 slots=8
node2 slots=8
node3 slots=8
node4 slots=8
↓
┌──────────────────────────────────────────────────────────────┐
│ 3. SSH/RSH로 각 노드에 프로세스 시작                         │
└──────────────────────────────────────────────────────────────┘
mpirun → ssh node1 "orted --hnp-uri ... &"
mpirun → ssh node2 "orted --hnp-uri ... &"
mpirun → ssh node3 "orted --hnp-uri ... &"
mpirun → ssh node4 "orted --hnp-uri ... &"

orted (Open MPI Runtime Environment Daemon):
각 노드에서 로컬 프로세스들을 관리
↓
┌──────────────────────────────────────────────────────────────┐
│ 4. MPI 초기화 및 통신 설정                                   │
└──────────────────────────────────────────────────────────────┘
각 프로세스:
├─ MPI_Init() 호출
├─ Rank 할당 받기
├─ Communicator 생성
└─ 다른 프로세스와 연결 설정
↓
┌──────────────────────────────────────────────────────────────┐
│ 5. 애플리케이션 실행                                         │
└──────────────────────────────────────────────────────────────┘
모든 프로세스가 동기화되어 main() 실행

```



## srun   = Slurm 범용 작업 런처 (MPI 포함)   
### srun 아키텍처 (Slurm 통합)
```
사용자 명령어:
srun --nodes=4 --ntasks-per-node=8 --mpi=pmix ./mpi_program

┌─────────────────────────────────────────────────────────────┐
│ Slurm Controller (slurmctld)                                │
│ ├─ 작업 스케줄링                                            │
│ ├─ 리소스 할당 (4 노드, 32 태스크)                          │
│ ├─ 우선순위 관리                                            │
│ └─ 노드 상태 모니터링                                       │
└─────────────────────────────────────────────────────────────┘
↓
┌───────────────────────────────────────────────┐
│ Node 1 - slurmd (Slurm 데몬)                  │
│ ┌──────────────────────────────────────────┐ │
│ │ PMIx 서버 시작                           │ │
│ │ └─ 프로세스 관리 및 통신 조정            │ │
│ │                                           │ │
│ │ Rank 0-7: ./mpi_program                  │ │
│ │ ├─ CPU/GPU 할당 (cgroup)                │ │
│ │ ├─ 메모리 제한                           │ │
│ │ └─ PMIx를 통한 MPI 초기화               │ │
│ └──────────────────────────────────────────┘ │
└───────────────────────────────────────────────┘

┌───────────────────────────────────────────────┐
│ Node 2 - slurmd                               │
│ ┌──────────────────────────────────────────┐ │
│ │ PMIx 서버                                │ │
│ │ Rank 8-15: ./mpi_program                 │ │
│ │ └─ Slurm이 리소스 관리                  │ │
│ └──────────────────────────────────────────┘ │
└───────────────────────────────────────────────┘

[Node 3, Node 4도 동일]

특징:
✓ 통합 리소스 관리
✓ 작업 큐잉 및 스케줄링
✓ 다중 사용자 환경 지원
✓ 리소스 격리 (cgroup)
✓ 로깅 및 어카운팅

```

### srun 프로세스 시작 (PMIx 사용)
```
┌──────────────────────────────────────────────────────────────┐
│ 1. srun 실행                                                 │
└──────────────────────────────────────────────────────────────┘
srun --nodes=4 --ntasks-per-node=8 --mpi=pmix ./mpi_program
↓
┌──────────────────────────────────────────────────────────────┐
│ 2. Slurm Controller에 작업 제출                              │
└──────────────────────────────────────────────────────────────┘
slurmctld:
├─ 사용 가능한 노드 검색
├─ 리소스 할당 (4 노드)
├─ Job ID 생성 (예: 12345)
└─ 각 노드의 slurmd에 명령 전송
↓
┌──────────────────────────────────────────────────────────────┐
│ 3. 각 노드의 slurmd가 PMIx 서버 시작                         │
└──────────────────────────────────────────────────────────────┘
Node 1 slurmd:
├─ PMIx 서버 시작
├─ cgroup 생성 (리소스 격리)
│  └─ CPU: 0-7
│  └─ Memory: 64GB
└─ 8개 프로세스 fork
↓
┌──────────────────────────────────────────────────────────────┐
│ 4. PMIx를 통한 MPI 초기화                                    │
└──────────────────────────────────────────────────────────────┘
각 프로세스:
├─ PMIx_Init() 호출
│  └─ PMIx 서버에 연결
├─ MPI_Init() 호출
│  └─ PMIx를 통해 프로세스 정보 교환
├─ Rank 및 통신 엔드포인트 획득
└─ 다른 프로세스와 연결
↓
┌──────────────────────────────────────────────────────────────┐
│ 5. 애플리케이션 실행                                         │
└──────────────────────────────────────────────────────────────┘
Slurm이 전체 프로세스 라이프사이클 관리

```


### 시나리오 1: 단일 노드에 모두 배치
```
srun -n 4 --mpi=pmix ./my_program

Node1: [Process0] [Process1] [Process2] [Process3]


    총 4개 프로세스
    노드 지정 없음 → Slurm이 자동으로 1개 노드에 모두 배치
    같은 노드 내에서 통신 (빠름, 공유 메모리 사용 가능)

```



### 시나리오 2: 여러 노드에 분산
```
srun -n 4 -N 2 --mpi=pmix ./my_program

Node1: [Process0] [Process1]
Node2: [Process2] [Process3]


    총 4개 프로세스
    -N 2: 2개 노드 사용 지정
    자동으로 균등 분배 (각 노드에 2개씩)
    노드 간 통신 필요 (네트워크 사용)

```



### ntasks-per-node 옵션
```
srun -n 4 --ntasks-per-node=1 --mpi=pmix ./my_program

Node1: [Process0]
Node2: [Process1]
Node3: [Process2]
Node4: [Process3]


    총 4개 프로세스
    --ntasks-per-node=1: 각 노드에 1개 프로세스만 배치
    결과적으로 4개 노드 사용
    노드당 1개씩 분산 배치


srun -n 4 -N 2 --mpi=pmix ./my_program

    4개 태스크를 2개 노드에 분산
    각 노드: 2개 프로세스

srun -n 8 -N 2-4 --mpi=pmix ./my_program

    8개 태스크를 2~4개 노드에 분산
    Slurm이 가용 노드에 따라 자동 결정

```




### 실제 리소스 사용 예시, 예시 1: 2개 노드, 노드당 4개 프로세스
```
srun -n 8 -N 2 --mpi=pmix ./my_program

    총 8개 프로세스
    2개 노드 사용
    각 노드: 4개 프로세스 (8 cores per processor)
    각 노드의 나머지 4개 코어는 유휴 상태

Node1: [P0] [P1] [P2] [P3]  (4 cores idle)
Node2: [P4] [P5] [P6] [P7]  (4 cores idle)

```



### GPU 학습 (각 GPU마다 1개 프로세스)
```
# 4개 노드, 각 8개 GPU
srun --nodes=4 \
--ntasks-per-node=8 \
--gpus-per-task=1 \
--mpi=pmix \
./gpu_training

결과:
Node1: 8개 프로세스 (각각 GPU 0-7 사용)
Node2: 8개 프로세스 (각각 GPU 0-7 사용)
Node3: 8개 프로세스 (각각 GPU 0-7 사용)
Node4: 8개 프로세스 (각각 GPU 0-7 사용)
총: 32개 프로세스

```

