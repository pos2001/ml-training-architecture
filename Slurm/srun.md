## srun   = Slurm 범용 작업 런처 (MPI 포함)   
### srun 아키텍처 (Slurm 통합)
```
사용자 명령어:
srun --nodes=4 --ntasks-per-node=8 --mpi=pmix ./mpi_program

┌─────────────────────────────────────────────────────────────┐
│ Slurm Controller (slurmctld)                                │
│ ├─ 작업 스케줄링                                            │
│ ├─ 리소스 할당 (4 노드, 32 태스크)                          │
│ ├─ 우선순위 관리                                            │
│ └─ 노드 상태 모니터링                                       │
└─────────────────────────────────────────────────────────────┘
↓
┌───────────────────────────────────────────────┐
│ Node 1 - slurmd (Slurm 데몬)                  │
│ ┌──────────────────────────────────────────┐ │
│ │ PMIx 서버 시작                           │ │
│ │ └─ 프로세스 관리 및 통신 조정            │ │
│ │                                           │ │
│ │ Rank 0-7: ./mpi_program                  │ │
│ │ ├─ CPU/GPU 할당 (cgroup)                │ │
│ │ ├─ 메모리 제한                           │ │
│ │ └─ PMIx를 통한 MPI 초기화               │ │
│ └──────────────────────────────────────────┘ │
└───────────────────────────────────────────────┘

┌───────────────────────────────────────────────┐
│ Node 2 - slurmd                               │
│ ┌──────────────────────────────────────────┐ │
│ │ PMIx 서버                                │ │
│ │ Rank 8-15: ./mpi_program                 │ │
│ │ └─ Slurm이 리소스 관리                  │ │
│ └──────────────────────────────────────────┘ │
└───────────────────────────────────────────────┘

[Node 3, Node 4도 동일]

특징:
✓ 통합 리소스 관리
✓ 작업 큐잉 및 스케줄링
✓ 다중 사용자 환경 지원
✓ 리소스 격리 (cgroup)
✓ 로깅 및 어카운팅

```

### srun 프로세스 시작 (PMIx 사용)
```
┌──────────────────────────────────────────────────────────────┐
│ 1. srun 실행                                                 │
└──────────────────────────────────────────────────────────────┘
srun --nodes=4 --ntasks-per-node=8 --mpi=pmix ./mpi_program
↓
┌──────────────────────────────────────────────────────────────┐
│ 2. Slurm Controller에 작업 제출                              │
└──────────────────────────────────────────────────────────────┘
slurmctld:
├─ 사용 가능한 노드 검색
├─ 리소스 할당 (4 노드)
├─ Job ID 생성 (예: 12345)
└─ 각 노드의 slurmd에 명령 전송
↓
┌──────────────────────────────────────────────────────────────┐
│ 3. 각 노드의 slurmd가 PMIx 서버 시작                         │
└──────────────────────────────────────────────────────────────┘
Node 1 slurmd:
├─ PMIx 서버 시작
├─ cgroup 생성 (리소스 격리)
│  └─ CPU: 0-7
│  └─ Memory: 64GB
└─ 8개 프로세스 fork
↓
┌──────────────────────────────────────────────────────────────┐
│ 4. PMIx를 통한 MPI 초기화                                    │
└──────────────────────────────────────────────────────────────┘
각 프로세스:
├─ PMIx_Init() 호출
│  └─ PMIx 서버에 연결
├─ MPI_Init() 호출
│  └─ PMIx를 통해 프로세스 정보 교환
├─ Rank 및 통신 엔드포인트 획득
└─ 다른 프로세스와 연결
↓
┌──────────────────────────────────────────────────────────────┐
│ 5. 애플리케이션 실행                                         │
└──────────────────────────────────────────────────────────────┘
Slurm이 전체 프로세스 라이프사이클 관리

```


### 
