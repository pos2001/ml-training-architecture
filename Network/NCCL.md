
### NCCL
```
NCCL = GPU 간 통신을 위한 NVIDIA 라이브러리

용도:
- 멀티 GPU 학습 시 데이터 동기화
- GPU 간 텐서 교환
- 분산 딥러닝 통신

통신 패턴:
├── All-Reduce: 모든 GPU에서 결과 집계
├── Broadcast: 한 GPU에서 모든 GPU로 전송
├── Reduce: 여러 GPU 결과를 하나로 합침
└── All-Gather: 모든 GPU 데이터 수집

```


## 통신 주체: GPU

### 한 개의 프로세스는 1개 이상의 GPU를 관리할 수 있다

### NCCL 구조
```
물리적 구조:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

노드 1 (P5.48xlarge):
  GPU:
    ├─ GPU 0 (H100)
    ├─ GPU 1 (H100)
    ├─ GPU 2 (H100)
    ├─ GPU 3 (H100)
    ├─ GPU 4 (H100)
    ├─ GPU 5 (H100)
    ├─ GPU 6 (H100)
    └─ GPU 7 (H100)

프로세스 배치:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

일반적인 방법: 프로세스당 1개 GPU (권장)
  프로세스 0 → GPU 0
  프로세스 1 → GPU 1
  프로세스 2 → GPU 2
  프로세스 3 → GPU 3
  프로세스 4 → GPU 4
  프로세스 5 → GPU 5
  프로세스 6 → GPU 6
  프로세스 7 → GPU 7

가능하지만 비권장: 프로세스당 여러 GPU
  프로세스 0 → GPU 0, 1, 2, 3
  프로세스 1 → GPU 4, 5, 6, 7

```


<img width="874" height="336" alt="image" src="https://github.com/user-attachments/assets/bc265d33-3483-42de-b33d-d876a6dbb7d9" />


### 
```
┌─────────────────────────────────────────────────────────┐
│                       노드 1                             │
│                                                          │
│  ┌──────────────┐  ┌──────────────┐                    │
│  │ 프로세스 0    │  │ 프로세스 1    │                    │
│  │              │  │              │                    │
│  │  제어 ↓      │  │  제어 ↓      │                    │
│  │  GPU 0       │  │  GPU 1       │                    │
│  └──────┬───────┘  └──────┬───────┘                    │
│         │                 │                             │
│         └───── NCCL 통신 (NVLink) ─────┘                │
└─────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────┐
│                       노드 2                             │
│                                                          │
│  ┌──────────────┐  ┌──────────────┐                    │
│  │ 프로세스 2    │  │ 프로세스 3    │                    │
│  │              │  │              │                    │
│  │  제어 ↓      │  │  제어 ↓      │                    │
│  │  GPU 0       │  │  GPU 1       │                    │
│  └──────┬───────┘  └──────┬───────┘                    │
│         │                 │                             │
│         └───── NCCL 통신 (NVLink) ─────┘                │
└─────────────────────────────────────────────────────────┘
         │                                   │
         └────────── NCCL 통신 (EFA) ────────┘

```





### 왜 프로세스당 1개 GPU가 일반적인가?
```
프로세스당 1 GPU (권장):
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

프로세스 0 → GPU 0
프로세스 1 → GPU 1
프로세스 2 → GPU 2
프로세스 3 → GPU 3

장점:
✅ 각 GPU가 독립적으로 동작
✅ NCCL이 GPU 간 직접 통신 최적화
✅ GIL (Global Interpreter Lock) 문제 없음
✅ 디버깅 쉬움
✅ 메모리 관리 명확

```




### AWS-OFI-NCCL의 역할:
```
┌─────────────────────────────────────────┐
│ NCCL 애플리케이션                        │
│ (PyTorch, TensorFlow, MXNet 등)         │
└─────────────────────────────────────────┘
↓
┌─────────────────────────────────────────┐
│ NCCL 라이브러리                          │
└─────────────────────────────────────────┘
↓
┌─────────────────────────────────────────┐
│ AWS-OFI-NCCL Plugin ← 이것이 핵심!       │
│ (NCCL → libfabric 변환)                 │
└─────────────────────────────────────────┘
↓
┌─────────────────────────────────────────┐
│ libfabric                               │
└─────────────────────────────────────────┘
↓
┌─────────────────────────────────────────┐
│ EFA (Elastic Fabric Adapter)            │
│ - 100 Gbps 네트워크                      │
│ - 낮은 지연시간                          │
└─────────────────────────────────────────┘

```





```

```





```

```





```

```




```

```
